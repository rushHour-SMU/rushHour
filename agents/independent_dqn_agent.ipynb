{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndependentDQNAgent:\n",
    "    \"\"\"DQN Agent responsible for environment interaction.\"\"\"\n",
    "    def __init__(self, state_size, action_size, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "\n",
    "    def select_action(self, state, model):\n",
    "        \"\"\"Select an action using an epsilon-greedy policy.\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state_tensor)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay epsilon for epsilon-greedy policy.\"\"\"\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IndependentDQNAgent\n",
    "\n",
    "---\n",
    "\n",
    "## 주요 기능\n",
    "- **Epsilon-Greedy 정책**: 무작위 탐험과 Q-값 기반 행동 선택을 결합하여 행동을 결정합니다.\n",
    "- **동적 Epsilon 감소**: 학습이 진행될수록 탐험 비율(`epsilon`)을 점진적으로 줄여 효율성을 높입니다.\n",
    "- **Q-네트워크 연동**: 외부 Q-네트워크 모델을 사용하여 최적의 행동을 선택합니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 클래스 메서드\n",
    "\n",
    "### `__init__(self, state_size, action_size, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01)`\n",
    "- **설명**: 에이전트를 초기화하며, 상태 크기, 행동 크기, epsilon 값 등 학습에 필요한 주요 파라미터를 설정합니다.\n",
    "- **파라미터**:\n",
    "  - `state_size` (int): 환경의 상태 크기.\n",
    "  - `action_size` (int): 에이전트가 선택할 수 있는 행동의 수.\n",
    "  - `epsilon` (float): 초기 탐험 비율 (기본값: `1.0`).\n",
    "  - `epsilon_decay` (float): 매 에피소드 후 epsilon 감소 비율 (기본값: `0.995`).\n",
    "  - `epsilon_min` (float): epsilon 값의 최소 한계 (기본값: `0.01`).\n",
    "\n",
    "---\n",
    "\n",
    "### `select_action(self, state, model)`\n",
    "- **설명**: 현재 상태와 Q-네트워크 모델을 기반으로 epsilon-greedy 정책을 사용하여 행동을 선택합니다.\n",
    "- **파라미터**:\n",
    "  - `state`: 현재 환경의 상태.\n",
    "  - `model`: Q-값을 출력하는 Q-네트워크.\n",
    "- **반환값**: 선택된 행동의 인덱스 (정수).\n",
    "- **로직**:\n",
    "  - `random.random() < epsilon`: 무작위 탐험 수행.\n",
    "  - 그렇지 않을 경우, Q-네트워크에서 가장 높은 Q-값을 가진 행동 선택.\n",
    "\n",
    "---\n",
    "\n",
    "### `decay_epsilon(self)`\n",
    "- **설명**: epsilon 값을 감소시켜 탐험 비율을 줄이고 활용을 강화합니다.\n",
    "- **로직**:\n",
    "  - 현재 `epsilon`이 `epsilon_min`보다 크다면, `epsilon_decay` 비율로 감소.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
