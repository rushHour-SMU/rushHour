{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork(nn.Module):\n",
    "    \"\"\"Deep Q-Network implemented in PyTorch.\"\"\"\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size * state_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)  # Flatten the input\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return (\n",
    "            torch.tensor(states, dtype=torch.float, device=device),\n",
    "            torch.tensor(actions, dtype=torch.long, device=device),\n",
    "            torch.tensor(rewards, dtype=torch.float, device=device),\n",
    "            torch.tensor(next_states, dtype=torch.float, device=device),\n",
    "            torch.tensor(dones, dtype=torch.float, device=device),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQNetwork 클래스\n",
    "\n",
    "## 개요\n",
    "`DQNetwork` 클래스는 PyTorch를 기반으로 구현된 심층 Q-네트워크(Deep Q-Network, DQN)입니다. 이 네트워크는 강화학습에서 에이전트의 Q-값을 계산하는 데 사용됩니다.\n",
    "\n",
    "## 메소드\n",
    "### `__init__(self, state_size, action_size)`\n",
    "- 입력 상태 크기와 가능한 행동 수를 기반으로 신경망을 초기화합니다.\n",
    "- 네트워크는 세 개의 선형 계층으로 구성되어 있습니다.\n",
    "\n",
    "### `forward(self, x)`\n",
    "- 신경망을 통과하는 입력 x를 처리하여 각 행동에 대한 Q-값을 출력합니다.\n",
    "- 입력 데이터는 평탄화되고, 각 계층은 ReLU 활성화 함수를 사용하여 처리됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReplayBuffer\n",
    "\n",
    "## 주요 기능\n",
    "- **경험 저장**:\n",
    "  - 환경과의 상호작용에서 발생하는 `(state, action, reward, next_state, done)` 데이터를 저장.\n",
    "  - 저장 공간의 크기를 제한하여 오래된 데이터를 삭제(FIFO 방식).\n",
    "- **배치 샘플링**:\n",
    "  - 저장된 데이터에서 무작위로 `batch_size`만큼 샘플링하여 학습 데이터를 제공합니다.\n",
    "- **현재 크기 확인**:\n",
    "  - 저장된 데이터의 개수를 반환.\n",
    "\n",
    "---\n",
    "\n",
    "## 클래스 메서드\n",
    "\n",
    "### `__init__(self, max_size)`\n",
    "- **설명**:\n",
    "  - 버퍼를 초기화하고 최대 크기를 설정합니다.\n",
    "- **파라미터**:\n",
    "  - `max_size` (int): 버퍼의 최대 크기. 초과 시 오래된 데이터를 삭제.\n",
    "- **구성 요소**:\n",
    "  - `self.buffer`: `deque`를 사용하여 FIFO 방식으로 데이터를 저장.\n",
    "\n",
    "---\n",
    "\n",
    "### `push(self, state, action, reward, next_state, done)`\n",
    "- **설명**:\n",
    "  - 새로운 경험 데이터를 버퍼에 저장합니다.\n",
    "- **파라미터**:\n",
    "  - `state`: 현재 상태.\n",
    "  - `action`: 에이전트가 선택한 행동.\n",
    "  - `reward`: 행동 수행 후 받은 보상.\n",
    "  - `next_state`: 행동 수행 후 도달한 다음 상태.\n",
    "  - `done`: 에피소드 종료 여부 (`True` 또는 `False`).\n",
    "- **특징**:\n",
    "  - 최대 크기를 초과하면 가장 오래된 데이터가 삭제됩니다.\n",
    "\n",
    "---\n",
    "\n",
    "### `sample(self, batch_size)`\n",
    "- **설명**:\n",
    "  - 저장된 경험 데이터에서 `batch_size`만큼 랜덤 샘플링합니다.\n",
    "- **파라미터**:\n",
    "  - `batch_size` (int): 반환할 샘플 데이터의 크기.\n",
    "- **반환값**:\n",
    "  - `states`, `actions`, `rewards`, `next_states`, `dones`:\n",
    "    - 각각 `torch.Tensor` 형식으로 반환되며, 학습 과정에서 입력 데이터로 사용.\n",
    "- **특징**:\n",
    "  - 무작위 샘플링을 통해 샘플 간 상관성을 줄여 안정성을 강화합니다.\n",
    "\n",
    "---\n",
    "\n",
    "### `__len__(self)`\n",
    "- **설명**:\n",
    "  - 버퍼에 저장된 경험 데이터의 개수를 반환합니다.\n",
    "- **반환값**:\n",
    "  - 현재 저장된 데이터의 개수 (`int`).\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
